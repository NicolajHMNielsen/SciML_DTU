---
title: What is in the $p$ dimensions?
author: Anton Ruby Larsen and Nicolaj Hans Nielsen
date: June 30th, 2021
---

```{julia,eval=false}
using Flux, DiffEqFlux, OrdinaryDiffEq, Optim, Plots, Sundials, ODEInterfaceDiffEq
```

## Introduction
Consider a forest where we monitor the rabbit and the wolf population and think
that only the interactions between these species determine the populations.
In reality, there are bears around that affect the population of the other two species.
With only the data from the rabbit and wolf populations, will the neural ODE be able
to encapsulate the dynamics even without knowledge of the existence of the
influence of the bears?

Say we use an augmented neural ODE with one added dimension,
would the dynamics of the added dimension resemble that of the real bear dynamics?

These are the questions we will address in this notebook. The agenda is

1. *Introduce bears to the Lotka-Volterra equation*

2. *See if the augmented neural ODE can encapsulate the dynamics without knowledge of the bears*

3. *Compare the real bear dynamics with that of the augmented NODE*

#### 1. Introducing bears

To be consistent with the notation used in [the introduction to ANODE](https://nicolajhmnielsen.github.io/SciML_DTU/ANODE/HTML/Augmented),
let `x` be *rabbits*, `y` be *wolfs* and `z` be *bears* and define the system of ODEs as


```math
\frac{\mathrm{d}}{\mathrm{d}t}x=Î±x-Î²xy-Î¶xz  \\
\frac{\mathrm{d}y}{\mathrm{d}t}=-Î³y+Î´xy-Î·yz \\
\frac{\mathrm{d}z}{\mathrm{d}t}=-Îºz+Î»xz+Î¼yz
```

which we can implement in julia

```{julia,eval=false}
function lotka_volt_bear(du,u,p,t)
  x, y, z = u
  Î± ,Î², Î¶, Î³, Î´, Î·, Îº, Î», Î¼= p'
  du[1] = dx = Î±*x - Î²*x*y - Î¶*x*z
  du[2] = dy =  - Î³*y + Î´*x*y  -Î·*y*z
  du[3] = dz = - Îº*z + Î»*x*z + Î¼*y*z
end
```


`p` is a matrix of the parameters. The values of the parameters for the interactions between
rabbits and wolfs are the same as in [the introduction to ANODE](https://nicolajhmnielsen.github.io/SciML_DTU/ANODE/HTML/Augmented)
and the parameters for the interaction of bears are chosen arbitrarily. We
consider $t=[0,15]$, use the Tsitouras 5/4 Runge-Kutta method, `Tsit5()` and save
`100` datapoints of the simulation.



```{julia,eval=false}
u0 = Float64[ 2; 1; 1.3]
p_ = [1.5 1 0.3;
     2 1 0.3;
     1.2 0.36 0.31]
tspan = (0.0, 15.0)
datasize = 100
t = range(tspan...,length=datasize)

prob = ODEProblem(lotka_volt_bear, u0,tspan, p_)
solution = solve(prob,Tsit5(), saveat = t)
plot(solution,label=["x" "y" "z"],xlabel="t",seriestype = :scatter,
     ylims=(0,3),legend=:bottomright, title="System with bears")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/introducing_bears.png?raw=true)

this is the system we will work with in the following.

## 2. Augmented NODE

We will now pretend we have no knowledge of the existence of bears or that we neglect
their effect on the populations of rabbits and wolves.

We explicitly take out only the rabbits and wolves:

```{julia,eval=false}
sol_ðŸº_ðŸ° = solution[1:2,:]
u0_ðŸº_ðŸ° = u0[1:2]
```

We will try to find the dynamics of the rabbits and wolves without including the dynamics of the
bears. To do that, we will use an ANODE with one added dimension and use the
architecture as in the [introduction](https://nicolajhmnielsen.github.io/SciML_DTU/ANODE/HTML/Augmented).


```{julia,eval=false}
input_dim = size(u0_ðŸº_ðŸ°)[1]
hidden_dim = 1
u0_aug = Float64[u0_ðŸº_ðŸ°;zeros(hidden_dim)]

NN = FastChain((x, p) -> x,
                  FastDense(input_dim+hidden_dim, 32, swish),
                  FastDense(32, 32, swish),
                  FastDense(32, input_dim+hidden_dim))

prob_neuralode = NeuralODE(NN, tspan, Tsit5(), saveat = t)

function predict_neuralode(p)
  Array(prob_neuralode(u0_aug, p))
end
```

We use square loss and note that we only take out the dimension of interest `pred[1:input_dim,:]`
for the loss function.

```{julia,eval=false}
function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, sol_ðŸº_ðŸ° .- pred[1:input_dim,:])
    return loss, pred
end
```

We will first do `1000` iterations with an `ADAM` optimizer with a learning rate of
`0.05`. Then we take out the minimizer and do `300` iterations of `BFGS` or until convergence.
We animate the training procedure with the `ADAM` and the `BFGS`
optimizer and make one coherent loss function per iteration.

```{julia, eval=false}
iter = 0
max_iter_adam = 1000
max_iter_BFGS = 300
loss_vec = Array{Float64}(undef,max_iter_adam+max_iter_BFGS+1)
anim_bears = Animation()
callback = function (p, l, pred)
  global list_plots, iter, anim_bears

  # response in training progression
  if iter%50 == 0
    display("Loss of iteration $iter: $l")
  end

  # save loss (+1 as julia is 1-indexed)
  loss_vec[iter+1] = l

  # plot current prediction against data
  plt = plot(t, (sol_ðŸº_ðŸ°)', label = ["x" "y"],seriestype = :scatter,
                legend=:bottomright,color=[:blue :red],xlabel="t",ylims=(-4,5.5))
  plot!(plt, t, pred', label = ["x NODE" "y NODE" "Aug dim"],color=[:blue :red :green])
  frame(anim_bears)

  iter += 1
  return false
end
result_neuralode_adam = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                          ADAM(0.05), cb = callback,
                                          maxiters = max_iter_adam);

# construct and save the animation as a gif
gif(anim_bears,raw"ANODE\Figures\Bears\bear_aug_1dim_adam.gif",fps=20)
```

We save the gif for training with `ADAM` and the result of the optimization `result_neuralode_adam`.

We take out the minimizer `result_neuralode_adam.minimizer` and use that as starting
point for the training with `BFGS`.


```{julia,eval=false}
# initialize animation again
anim_bears = Animation()
result_neuralode_BFGS = DiffEqFlux.sciml_train(loss_neuralode,
                                          result_neuralode_adam.minimizer,
                                          BFGS(initial_stepnorm=0.05),
                                          cb = callback,
                                          maxiters = 300);
gif(anim_bears,raw"ANODE\Figures\Bears\bear_aug_1dim_bfgs_3.gif",fps=10)
```

First, we examine the log-loss function

```{julia,eval=false}
plt_loss = plot(loss_vec[1:iter],
                yaxis=:log,
                label="log loss",
                title="Log loss function",
                xlabel="Iteration",
                ylabel="log loss")
# a need command used throughout to save plots
png(plt_loss,raw"ANODE\Figures\Bears\loss_bears_aug_1dim_3.png")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/loss_bears_aug_1dim_3.png?raw=true)

During the first `1000` iterations with the `ADAM` optimizer, we see some fluctuations in the loss functions
as it tries new neighborhoods, however, we also see longs periods of stagnation.
After 1000 iterations we see how the `BFGS` optimizer
steadily decreases the loss function.

##### Animation of training

In this `gif` we see how the `ADAM` optimizer tries out multiple parameter settings but
also gets stuck in long periods of time
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/bear_aug_1dim_adam_3.gif?raw=true)

the `BFGS` seems to more steadily find parameters to best fit the data.

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/bear_aug_1dim_bfgs_3.gif?raw=true)

In the end, we encapsulate the rabbit and wolf dynamics really good
and then the added dimension moves around a bit arbitrarily.

```{julia,echo=false,eval=false}
opt_par = result_neuralode_BFGS.minimizer
io = open("params.txt", "w") do io
  for x in opt_par
    println(io, x)
  end
end
predictions = predict_neuralode(result_neuralode_BFGS.minimizer)
```

We can also consider the dynamics in the $(x,y)$ phase-diagram. To make the plot
a bit more informative, we include the time dimension in a gif.

```{julia,eval=false}
anim_phase = @animate for i=1:100
    plot(solution[1,1:i],solution[2,1:i],
    label="true",
    xlabel="x",
    ylabel="y",
    seriestype = :scatter,
    ylims=(0.8,1.44),
    xlims=(1.9,2.85))
    plot!(predictions[1,1:i], predictions[2,1:i],
    label="ANODE")
end
gif(anim_phase,raw"ANODE\Figures\Bears\phase_dyn.gif",fps=7)
```


![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/phase_dyn.gif?raw=true)

Here we see in phase space that we can encapsulate the dynamics of the rabbits and wolfs
quite well without knowledge and data from the bear population.

*Note: it seems that
the NODE does linear interpolation. This is not the case. It is because in the above
we only save the predictions of the neural network at discrete time points.*


## 3. Compare added dimension with dynamics of bears

We now plot the dynamics of the orginal system and that of the found NODE with the
bear dynamics unknown

```{julia, eval=false,echo=false}
plt_sys = plot(t,solution[1:3,:]',
                label=["x" "y" "z"],
                xlabel="t",
                seriestype = :scatter,
                ylims=(0,4.5),
                legend=:topright,
                title="Comparison of the dynamics")
plt_pred = plot!(plt_sys, t, predictions',
                label = ["x NODE" "y NODE" "Aug dim"],
                color=[:blue :red :green],
                lw=3)
png(plt_pred,raw"ANODE\Figures\Bears\compare_dynamics.png")

plt_bear = plt_sys = plot(t,[solution[3,:],predictions[3,:]],
                label=["z" "Aug dim"],
                xlabel="t",
                seriestype = [:scatter :line],
                color = [:green :blue],
                ylims=(0,2),
                legend=:bottomright,
                title="z and augmented dimension",
                lw=3)

png(plt_bear,raw"ANODE\Figures\Bears\compare_dynamics_bears_only.png")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/compare_dynamics.png?raw=true)
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/compare_dynamics_bears_only.png?raw=true)


The dynamics in the augmented dimension is not directly the same as that of the bears.
A closer look reveals that the period of the oscillations seems to be close, the magnitude is not close,
and the phase is slightly shifted.

It seems that the dynamics in the augmented dimension is not entirely random but
neither can it be used to directly find the dynamics of external or unknown variables
of the system.




## Discussion

To test the stability and robustness of the NODE model, we will see what happens if we
- **A**: adjust the initial condition
- **B**: simulate beyond $t=[0,15]$.

#### **A**. Intial conditions
Here we will try to change the initial conditions to see how it affects the prediction
of the NODE. First, we try to bias the model to better encapsulate the bear dynamics
and enforce the correct initial condition for the bears:

```{julia, eval=false}
prediction_init_bear = Array(prob_neuralode(u0, result_neuralode_BFGS.minimizer))
plt_sys_init = plot(t,solution[1:3,:]',
                label=["x" "y" "z"],
                xlabel="t",
                seriestype = :scatter,
                ylims=(0,4.5),
                legend=:topright,
                title="Correct initial condition for added dimension")
plt_pred_init = plot!(plt_sys_init, t, prediction_init_bear',
                label = ["x NODE" "y NODE" "Aug dim"],
                color=[:blue :red :green],
                lw=3)
png(plt_pred_init,raw"ANODE\Figures\Bears\init_same_bear.png")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/init_same_bear.png?raw=true)

The correct initial condition does not resolve the wrong magnitude in the oscillations
of the added dimension, however, it makes our added dimension start at the right spot.

In [^Strauss] they also enforce the correct bear initial condition during training but with significant
increase in similarity between the bear and added dimension.


We now try 3 different initial conditions. We increase the initial condition for each of the
variables - one at a time - and fix the others. We trained with $u_0=\{2,1,1.3\}$ and now try

1. Set $x_0=3$   s.t.  $u_0=\{3,1,1.3\}$
2. Set $y_0=1.5$ s.t.  $u_0=\{2,1.5,1.3\}$
3. Set $z_0=2$   s.t.  $u_0=\{2,1,2\}$


```{julia, echo=false,eval=false}

u0_init = Float64[3; 1 ;  1.3;
                  2; 1.5; 1.3;
                  2; 1  ; 2]
prob_init = ODEProblem(lotka_volt_bear, u0_init[7:9],tspan, p_)
solution_init = solve(prob_init,Tsit5(), saveat = t)

prediction_init = Array(prob_neuralode([u0_init[7:8];0], result_neuralode_BFGS.minimizer))

plt_sys_init = plot(t,solution_init[1:3,:]',
                label=["x" "y" "z"],
                xlabel="t",
                seriestype = :scatter,
                ylims=(0,5),
                legend=:topright,
                title="\$z_0=2\$")
plt_pred_init = plot!(plt_sys_init, t, prediction_init',
                label = ["x NODE" "y NODE" "Aug dim"],
                color=[:blue :red :green],
                lw=3)

png(plt_pred_init,raw"ANODE\Figures\Bears\init_z0_2.png")
```

###### 1.
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/init_x0_3.png?raw=true)
###### 2.
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/init_y0_1d5.png?raw=true)
###### 3.
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/init_z0_2.png?raw=true)

The initial condition were arbitrarily chosen, however, we see that the model cannot
perfectly generalize to new initial conditions and when we tested very different initial
conditions, the predictions were even worse. This suggests that one could construct multiple trajectories
with different initial conditions for the model to train on and hopefully be able to generalize better.

#### **B** Long time behavior
We now check what happens if set $t=[0,70]$ without retraining to see the long time
behavior

```{julia, eval=false,echo=false}
u0 = Float64[ 2; 1; 1.3]
p_ = [1.5 1 0.3;
     2 1 0.3;
     1.2 0.36 0.31]
tspan_long = (0.0, 70.0)
datasize = 1000
t_long = range(tspan_long...,length=datasize)

prob = ODEProblem(lotka_volt_bear, u0,tspan_long, p_)
solution_long = solve(prob,Tsit5(), saveat = t_long)


prob_neuralode_long = NeuralODE(NN, tspan_long, Tsit5(), saveat = t_long)
prediction_long = Array(prob_neuralode_long(u0_aug, result_neuralode_BFGS.minimizer))
```

```{julia, eval=false,echo=false}
plt_sys_long = plot(t_long,solution_long[1:3,:]',
                label=["x" "y" "z"],
                xlabel="t",
                seriestype = :scatter,
                ylims=(0,4.5),
                legend=:topright,
                title="Comparison of the dynamics")
plt_pred_long = plot!(plt_sys_long, t_long, prediction_long',
                label = ["x NODE" "y NODE" "Aug dim"],
                color=[:blue :red :green],
                lw=3)
png(plt_pred_long,raw"ANODE\Figures\Bears\compare_dynamics_70.png")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/compare_dynamics_70.png?raw=true)

First, inspect the original system to see that after $t=20$, the bear population
increases steadily and the wolf population starts to decrease. Remember that the
model was trained in $t=[0,15]$ hence the model was not presented to data with
increasing bear population.

The trained ANODE quickly starts to deviate in the augmented dimension for $t>15$
and in turn, the rabbit and wolf populations also start to differ. An obvious way to
improve this could be to train on data with a longer time span.

If more data is not available one could use other techniques to reduce overfitting
for neural networks e.g.
- use dropouts
- use stopping criteria. *remember at the end of the training gif for `BFGS` where the added dimension fluctuated randomly*
- use ensemble methods and train multiple ANODEs simultaneously and take the mean of the predictions
- if we had more data we could use different cross-validation techniques for time series



# References
[^Strauss]: Strauss, Robert *Augmenting Neural Differential Equations to Model Unknown Dynamical Systems with Incomplete State Information*, arXiv:2008.08226, 2020
