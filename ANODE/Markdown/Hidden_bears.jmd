---
title: What is in the $p$ dimensions
author: Anton Ruby Larsen and Nicolaj Hans Nielsen
date: June 30th, 2021
---

```{julia,eval=false}
using Flux, DiffEqFlux, OrdinaryDiffEq, Optim, Plots, Sundials, ODEInterfaceDiffEq
```

Consider a forest where we monitor the rabbit and the wolf population because we
think they are the most vital part of the system and because they are easy to monitor.
However, in reality there are hidden bears around that greatly affects the population
of the other two species.

With only the data from the rabbit and wolf populations, will he neural ODE be able
to encapsulate the dynamics even without knowledge of the existance of the
extra variable? Say we use a augmented neural ODE with one added dimension,
would the dynamics of the added dimension resemble that of the real bear dynamics?

In the following we will:

1. Introduce bears to the Lotka-Volterra equation

2. See if the augmented neural ODE can encapsulate the dynamics with bears as
an unknown variable

3. Compare the real bear dynamics with that of the augmented NODE

## 1. Introducing bears

We will now Introduce bears and to be consistant with the notation let `x` be
*rabbits*, `y` be *wolfs* and `z` be *bears* and define the system as:


```math
\frac{\mathrm{d}}{\mathrm{d}t}x=Œ±x-Œ≤xy-Œ∂xz  \\
\frac{\mathrm{d}y}{\mathrm{d}t}=-Œ≥y+Œ¥xy-Œ∑yz \\
\frac{\mathrm{d}z}{\mathrm{d}t}=-Œ∫z+Œªxz+Œºyz
```


we can directly define a function in julia:

```{julia,eval=false}
function lotka_volt_bear(du,u,p,t)
  x, y, z = u
  Œ± ,Œ≤, Œ∂, Œ≥, Œ¥, Œ∑, Œ∫, Œª, Œº= p'
  du[1] = dx = Œ±*x - Œ≤*x*y - Œ∂*x*z
  du[2] = dy =  - Œ≥*y + Œ¥*x*y  -Œ∑*y*z
  du[3] = dz = - Œ∫*z + Œª*x*z + Œº*y*z
end
```

here `p` is a matrix which is defined s.t. one can easily adjust the parameters.
Here we plot with the same parameteres as that of the Lotka-Volterra system and
some arbitraraly chosen parameters for the interaction of bears for $t=[1,10]$:



```{julia,eval=false}
u0 = Float64[ 2; 1; 1.3]
p_ = [1.5 1 0.3;
     2 1 0.3;
     1.2 0.36 0.31]
tspan = (0.0, 15.0)
datasize = 100
t = range(tspan...,length=datasize)

prob = ODEProblem(lotka_volt_bear, u0,tspan, p_)
solution = solve(prob,Tsit5(), saveat = t)
plot(solution,label=["x" "y" "z"],xlabel="t",seriestype = :scatter,ylims=(0,3))
```

## 2. Augmented NODE

We will now pretend we have no knowledge of the existance of bears or we might not
have anticipated that they affected our dynamical system. We will, therefore, try
to encapuslate the dynamics of the rabbits and wolfs without including dynamics of the
bear. To do that, we will use an ANODE with one added dimension.

We explicitly take out only the rabbits and wolfs:

```{julia,eval=false}
sol_üê∫_üê∞ = solution[1:2,:]
u0_üê∫_üê∞ = u0[1:2]
```

We define the architecture which is the same as in the [introduction](https://nicolajhmnielsen.github.io/SciML_DTU/ANODE/HTML/Augmented).


```{julia,eval=false}
input_dim = size(u0_üê∫_üê∞)[1]
hidden_dim = 1
u0_aug = Float64[u0_üê∫_üê∞;zeros(hidden_dim)]

NN = FastChain((x, p) -> x,
                  FastDense(input_dim+hidden_dim, 32, swish),
                  FastDense(32, 32, swish),
                  FastDense(32, input_dim+hidden_dim))

#prob_neuralode = NeuralODE(NN, tspan, Tsit5(), saveat = dt)
prob_neuralode = NeuralODE(NN, tspan, Tsit5(), saveat = t)

function predict_neuralode(p)
  Array(prob_neuralode(u0_aug, p))
end
```

We use square loss and note we only do it until the input dimension as the augmented
dimension should not be part of the loss function.

```{julia,eval=false}
function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, sol_üê∫_üê∞ .- pred[1:input_dim,:])
    return loss, pred
end
```

We now initialize an animation and set out max number of itarations to `1000` but
with a max error of 2 using `adam`. If we find something below 2, we will switch
and use `BFGS`:


```{julia, eval=false}
iter = 0
max_iter_adam = 1000
max_iter_BFGS = 300
loss_vec = Array{Float64}(undef,max_iter_adam+max_iter_BFGS+1)
anim_bears = Animation()
callback = function (p, l, pred)
  global list_plots, iter, anim_bears

  # response in training progression
  if iter%50 == 0
    display("Loss of iteration $iter: $l")
  end

  # save loss (+1 as julia is 1-indexed)
  loss_vec[iter+1] = l

  # plot current prediction against data
  plt = plot(t, (sol_üê∫_üê∞)', label = ["x" "y"],seriestype = :scatter,
                legend=:bottomright,color=[:blue :red],xlabel="t",ylims=(-4,5.5))
  plot!(plt, t, pred', label = ["x NODE" "y NODE" "Aug dim"],color=[:blue :red :green])
  frame(anim_bears)

  iter += 1
  return false
end
result_neuralode_adam = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                          ADAM(0.05), cb = callback,
                                          maxiters = max_iter_adam);
gif(anim_bears,raw"ANODE\Figures\Bears\bear_aug_1dim_adam_3.gif",fps=20)
```

We save the animation in the last line. Then we intialize the animation agian
to see the training for the 300 iterations with `BFGS`. We will takeout the
minimizer to far and use that


```{julia,eval=false}
anim_bears = Animation()
result_neuralode_BFGS = DiffEqFlux.sciml_train(loss_neuralode,
                                          result_neuralode_adam.minimizer,
                                          BFGS(initial_stepnorm=0.05),
                                          cb = callback,
                                          maxiters = 300);
gif(anim_bears,raw"ANODE\Figures\Bears\bear_aug_1dim_bfgs_3.gif",fps=10)
```



Something very interesing happend during the training which we can see in the loss
function:

```{julia,eval=false}
plt_loss = plot(loss_vec[1:iter],
                yaxis=:log,
                label="log loss",
                title="Log loss function",
                xlabel="Iteration",
                ylabel="log loss")
png(plt_loss,raw"ANODE\Figures\Bears\loss_bears_aug_1dim_3.png")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/loss_bears_aug_1dim.png?raw=true)

We see that the errors drops drastically using adam and after 1000 iterations where
we use the `BFGS` it also steady decreases. Visually the training with `ADAM` and
the first 1000 iterations looks like:


![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/bear_aug_1dim_adam.gif?raw=true)

and for the `BFGS`

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/bear_aug_1dim_bfgs.gif?raw=true)



We see that have succesfully encapsulated the dynamics of the rabits and the wolfs
 without knowing the existance of the bears.

```{julia,echo=false,eval=false}
io = open("params.txt", "w") do io
  for x in opt_par
    println(io, x)
  end
end
```

```{julia,echo=false,eval=false}
opt_par = result_neuralode_BFGS.minimizer
predictions = predict_neuralode(result_neuralode_BFGS.minimizer)


```
## 3. Compare added dimension with dynamics of bears

We now plot the dynamics of the orginial system and that of the found NODE with the
bear dynamics unknown

```{julia, eval=false,echo=false}
plt_sys = plot(t,solution[1:3,:]',
                label=["x" "y" "z"],
                xlabel="t",
                seriestype = :scatter,
                ylims=(0,4.5),
                legend=:topright,
                title="Comparison of the dynamics")
plt_pred = plot!(plt_sys, t, predictions',
                label = ["x NODE" "y NODE" "Aug dim"],
                color=[:blue :red :green],
                lw=3)
png(plt_pred,raw"ANODE\Figures\Bears\compare_dynamics.png")

plt_bear = plt_sys = plot(t,[solution[3,:],predictions[3,:]],
                label=["z" "Aug dim"],
                xlabel="t",
                seriestype = [:scatter :line],
                color = [:green :blue],
                ylims=(0,2),
                legend=:bottomright,
                title="z and augmented dimension",
                lw=3)

png(plt_bear,raw"ANODE\Figures\Bears\compare_dynamics_bears_only.png")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/compare_dynamics.png?raw=true)
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/Bears/compare_dynamics_bears_only?raw=true)


We see that the dynamics we get in the augmented dimension does not directly the
same as the bears. The period of the oscilations seems to be close but the magnitudes
are not.

## Discussion

We see that elements of the bear dynamics is encapsulated and therefore one could
try to bias the model to better do so. In [^Strauss] they enforce the correct bear
 initial condition instead of just 0 for the augmented dimension but with no significant effect.

It seems that the dynamics in the augmented dimension is not entirely random but
neither can it be used to directly find the dynamics of external or unknown variables
of the system.



# References
[^Strauss]: Strauss, Robert *Augmenting Neural Differential Equations to Model Unknown Dynamical Systems with Incomplete State Information*, arXiv:2008.08226, 2020
