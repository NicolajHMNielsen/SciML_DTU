---
title: Augmented Neural ODE
author: Anton Ruby Larsen and Nicolaj Hans Nielsen
date: February 12th, 2021
---

Initialize the needed packages in julia:

```{julia,eval=false}
using Flux, DiffEqFlux, OrdinaryDiffEq, Optim, Plots
```

In the initial analysis, we found that our neural ordinary differential equations, **NODE**,
sometimes suffered from long training time, instability in the training, and sometimes
the result was inaccurate. How come this be?

* Is this because the NODE simply cannot represent the function?

* If that is not the case, is there a way for us to improve the training of the NODE?

In the following, we will address the first question with theoretical considerations
and then introduce the augmented neural ordinary differential equation to see if it can
alleviate our problems with training.

## 1. Functions ODEs cannot represent
NODEs are essentially an ODE where the function that proscribes the dynamics
is a neural network. From the *universal approximation theorem*, we know that
neural networks can theoretically approximate any continuous to arbitrary precision.
When these are built into the framework of ODEs, the NODE inherits limitations and
properties of generic ODEs.

#### Existence and uniqueness for ODEs
Trajectories of ODEs **cannot** intersect in phase-space. This follows directly
from the existence and uniqueness theorem, section 2.2, [^Perko]. The proof can
be seen in *A.1*, [^Dupont].

A simple way to understand this is to look at an example in 1-D ODE.

Consider two solutions $H_1(t)$ and $H_2(t)$ with different intial condition $H_1(t_0)\neq H_2(t_0)$
in some timespan $t=[t_0, t_1]$. Further, let $P$ be an instance at time $t=t_b$ where
 $H_1(t_p)=H_2(t_p)$. The instance is depicted below

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/unique.png?raw=true)

The intersection at $t=t_p$ is problematic and a direct violation of the existence and uniqueness theorem.
**The reason** it is problematic can easily be seen if we start at $t_1$ and move
back in time for each of the trajectories. This seems fine until we reach
$t=t_p$ where the solutions intersect. None of the trajectories are unique and
if we were to continued towards $t_0$, we would not be able to know which trajectory
to follow. This ultimately means that we could end up at arbitrary
initial conditions. Likewise different initial conditions could lead us down arbitrary
trajectories for $t_0 \rightarrow t_1$ which would break down many physical systems
derived from first principles as they heavily rely on the existence and uniqueness of our solutions.

#### Existence and uniqueness tested on neural ODEs
As NODEs are built into the framework of ODEs, the existence and uniqueness theorem
also holds here. To test this, we will present the model would for data that would obviously be
well approximated by a function that violates the existence and uniqueness theorem.
Could this BlackBox somehow shortcircuit the theorem?

**Test of existence and uniqueness for NODE**
*NOTE: it is quite a special case, hence the implementation might not seem
entirely intuitive. Therefore, focus mainly on the graphical results*

Consider some simple 1-D system and with two sets of data points

$$
\begin{align*}
r &= \{(0,1),(1,-1)\} \\
b &= \{(0,-1),(1,1)\}
\end{align*}
$$


```{julia, eval=false}
r = Float32[1 -1]
b = Float32[-1 1]
data = vcat(r,b) # Float32[1.0 -1.0;
                 #        -1.0 1.0]
plot(0:1, data', label = ["b" "r"],xlabel="t",seriestype = :scatter,legend=:left,
     markersize=5,color=[:blue :red],ylims=(-1.2,1.2))
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/unique_setup.png?raw=true)

We now try to train a function that simultaneously find a flow that maps $r$ and $b$ at
time $t=0$ to the values at $t=1$. If the violation is not obvious, take a blue pencil in one hand and a red
pencil in the other hand. Put down the pencils at $t=0$ and connect the blue dots
and the red dots without lifting the pencils or crossing the lines.
That is impossible to do well without any intersection of trajectories.
However, we let the train the Neural ODE for 20 epochs to see what happens.

First, define the neural ODE problem in julia

```{julia, eval=false}
tspan = (0.0f0, 1f0)
tsteps = (0.0f0:0.01f0:1f0)
datasize = length(tsteps)

dudt2 = FastChain((x, p) -> x,
                  FastDense(1, 50, tanh),
                  FastDense(50, 1))
prob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)
```
then we define the prediction function where we simultaneously predict trajectories
for $r$ and $b$

```{julia, eval=false}
function predict_neuralode(p)
  vcat(Array(prob_neuralode(Array([r[1]]), p)),
      Array((prob_neuralode(Array([b[1]]), p)))
      )
end
```

We only want the loss at the first and last values, i.e., at $t=\{0,1\}$:

```{julia, eval=false}
function loss_neuralode(p)
    pred = predict_neuralode(p)
    # note modified only to take out the first and last
    loss = sum(abs2, data .- pred[:,[1,end]])
    return loss, pred
end
```

We define a call back function, i.e., a function we call by the end of each
training epoch to track our training progression. At each epoch, we display the
values on which we train with the trajectories of our model so far. Each plot we
make into a frame in an animation, `anim`.

```{julia, eval=false}
iter = 0
anim = Animation()
callback = function (p, l, pred; doplot = false)
  global list_plots, iter, anim

  # display the loss in console at each 5. epoch
  if iter%5 == 0
  display("Loss of iteration $iter: $l")
  end

  # plot current prediction against data
  plt = plot(0:1, data', label = ["b" "r"],xlabel="t",seriestype = :scatter,
            legend=:top,color=[:blue :red],ylims=(-1.2,1.2))
  plot!(plt, tsteps, pred', label = ["b NODE" "r NODE"],legend=:top,
        color=[:blue :red])

  # make the plot a frame in the animation
  frame(anim)

  iter += 1

  return false
end
```

We then train for at most 20 epochs with an `ADAM` optimizer with a learning rate
of `0.05`

```{julia, eval=false}
result_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                          ADAM(0.05), cb = callback,
                                          maxiters = 20);
```
```{julia,echo=false}
display("Loss of iteration 0: 7.4559307")
display("Loss of iteration 5: 2.1491983")
display("Loss of iteration 10: 2.01211")
display("Loss of iteration 15: 2.0020065")
display("Loss of iteration 20: 2.0007248")
```

Did the NODE allow trajectories to cross? This we check using the computed animation
for the 20 training epochs

```{julia, eval=false}
gif(anim,raw"ANODE\Figures\test_Violate.gif",fps=5)
```
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/test_Violate.gif?raw=true)



The flow determined by the NODE does not allow the intersection of the trajectories.
This demonstrates that the NODE has the inherent properties of the ODE and obeys
the existence and uniqueness theorem.

This is not always the case for ML algorithms that tries to mimic ODE behavior
and properties. In the report, we discussed ResNet and RNN and that NODE can be seen
as a way to parameterize the continuous transformation of the hidden states. In [^Dupont] they
use the same toy example as above and show that the ResNet allows trajectories to cross.


**In the part above** we conclude that some functions cannot be approximated by
a NODE due to the inherent properties of ODE flows. However, if we know that the
data source, we try to formulate a model for could be approximated by an ODE, then we can
use the NODE. Now we can try to improve the training process with the introduction of
the augmented neural ODE.


## 2. Introducing augmented neural ODE

When we train the neural network to approximate the dynamics of an ODE, the procedure
runs many different combinations of weights and biases. As trajectories cannot
cross in phase-space, we constraint the set of possible values of weights and biases.
As the training is gradient-based, this means that sometimes we end up at local minima
we cannot escape which in turn leads to poor approximations, huge computational cost, and complicated flows [^Dupont].
To extend the possible combinations of weights and biases, we expand the space on which
we learn the ODE flow. We lift the trajectories from ${\rm I\!R}^{d}$
to ${\rm I\!R}^{d+p}$. In the extra $p$ dimensions the model has a larger playground which
leads to simpler flows, more stable training, and increased accuracy.
This is very well described and introduced in [^Dupont]. Experimentally the augmented Neural ODE has
been shown to reduce the loss function, reduce the computational cost, improve stability and
generalization when presented to different initial conditions [^Dupont][^Strauss].


**Formally**
Consider a NODE formulated in terms of an initial value problem, *IVP*

```math
\frac{\mathrm{d}}{\mathrm{d}t}u(t)=f(u(t),t),
\quad \quad \quad u(0)=u_0, \quad u_0 \in{\rm I\!R}^{d}
```


here $f$ is a neural network.

We then introduce another IVP

```math
\frac{\mathrm{d}}{\mathrm{d}t}a(t)=f(a(t),t),
\quad \quad \quad a(0)=0, \quad a_0 \in{\rm I\!R}^{p}
```

then we vertically concatenate both of the IVP problems to obtain the augmented NODE problem:


$$
\left[\begin{array}{c} u(t) \\ a(t) \end{array}\right] = f\left(\begin{array}{c} u(t) \\ a(t) \end{array},t\right),
\quad \quad \quad \left[\begin{array}{c} u(0) \\ a(0) \end{array}\right]=\left[\begin{array}{c} u_0 \\ 0 \end{array}\right]
$$

We can now train this ANODE and only make the predicted $u(t)$
contribute to the loss function as this is the, $d$, dimensions in which our data
reside. We test this in an example with the Lotka-Volterra:

#### Example with Lotka-Volterra

We will now use the Lotka-Volterra to showcase how the ANODE works and look at the
training pattern. We first define the Lotka-Volterra model, make
a simulation, and test how the NODE performs on the data from the simulation. Then we will define the
augmented ODE problem and see how the ANODE performs and explore the training pattern
when presented to the same data.


***Define Lotka-Volterra model***

First, define the Lotka-Volterra with parameters $\left[\alpha, \beta, \gamma, \delta \right]=[1.5, 1, 2, 1]$
and intial conditions $u_0=\left[1, 1\right]^\intercal$ in the timespan $t=[0,15]$. Then we
take out `100` datapoints from the simulation:

```{julia,eval=false}
function lotka_volt!(du, u, p, t)
    x, y= u
    α ,β, γ, δ = p'

    du[1] = dx = α*x - β*x*y
    du[2] = dy =  - γ*y + δ*x*y
end

u0 = Float32[1.0; 1.0]
tspan = (0.0f0,15.0f0)
datasize = 100
t = range(tspan...,length=datasize)

# Define the experimental parameters
#p_ = Float32[α  β;
#             γ  δ]
p_ = Float32[1.5 1.0;
             2.0 1.0]

prob = ODEProblem(lotka_volt!, u0,tspan, p_)
sol_lotka = solve(prob, Tsit5(),saveat=t)
plot(t,sol_lotka',label=["x" "y"],xlabel="t",seriestype = :scatter,ylims=(0,6))
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/lotka_volt_bears.png?raw=true)

This is the dynamics we will try to capture with our NODE.

###### Lotka-Volterra with NODE

We then define our Neural ODE, the prediction function, and the loss function. We use
$2-32-2$ network with `swish` activation functions. For the ODE solver, we use a *Tsitouras 5/4 Runge-Kutta method*,
`tsit5()`, and save it only at time steps where we have data points. We then construct
the prediction function and use square loss.

```{julia, eval=false}
NN = FastChain((x, p) -> x,
                  FastDense(2, 32, swish),
                  FastDense(32, 32, swish),
                  FastDense(32, 2))

prob_neuralode = NeuralODE(NN, tspan, Tsit5(), saveat = t)

function predict_neuralode(p)
  Array(prob_neuralode(u0, p))
end

function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, sol_lotka .- pred)
    return loss, pred
end
```

We now initialize the iterator and an animation. We also modify
the call back function s.t. it only outputs the loss function every 50th iteration.
Further, we set a maximum of `600` iterations of training epochs:

```{julia, eval=false}
iter = 0
max_iter = 600
loss_vec = Array{Float32}(undef,max_iter+1)
anim_lotka = Animation()

callback = function (p, l, pred)
  global list_plots, iter, anim_lotka

  # response in training progression
  if iter%50 == 0
    display("Loss of iteration $iter: $l")
  end

  # save loss (+1 as julia is 1-indexed)
  loss_vec[iter+1] = l

  # plot current prediction against data
  plt = plot(t, sol_lotka', label = ["x" "y"],seriestype = :scatter,
            legend=:topright,color=[:blue :red],xlabel="t",ylims=(-0.1,5.5))
  plot!(plt, t, pred', label = ["x NODE" "y NODE"],color=[:blue :red])
  frame(anim_lotka)

  iter += 1
  return false
end

```

We use an `ADAM` optimizer and set a learning rate of `0.05`


```{julia, eval=false}
result_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                          ADAM(0.05), cb = callback,
                                          maxiters = max_iter);
```

We display the loss over iterations:

```{julia,eval=false}
loss_vec_copy = copy(loss_vec)
plt_loss = plot(loss_vec,yaxis=:log,label="log loss", title="Log of square loss function"
                ,xlabel="Iteration",ylabel="log loss")
png(plt_loss,raw"ANODE\Figures\loss_lotka-volterra.png")
```

![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/loss_lotka-volterra.png?raw=true)

We can now see the training as a gif:

```{julia, eval=false}
gif(anim_lotka,raw"ANODE\Figures\lotka-Volterra.gif",fps=15)
```
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/lotka-Volterra.gif?raw=true)

This is a problem we have often encounter when fitting oscillating systems. The
neural ODE simply takes the average of the oscillations and gets caught. However, then it suddenly spikes
up and finds some quite good estimate but then jumps away again as seen around iteration
200. This is both due to the limited set of weights and biases the network can attain
and because we use an `ADAM` optimizer that tries to explore new neighborhoods.

*Note: In the next [notebook](https://nicolajhmnielsen.github.io/SciML_DTU/ANODE/HTML/Hidden_bears) we
will also try to take out the minimizer from training using `ADAM` and use `BFGS` afterward. In this
notebook, we only consider the `ADAM` optimizer to explore the training patterns.*

###### Lotka-Volterra with ANODE and $p=1$

We now augment the ODE and add another dimension, i.e., set $p=1$. In practice, we pad the initial
conditions with 0 and change the architecture to a  $3-32-3$  network.

```{julia, eval=false}
input_dim = size(u0)[1]
hidden_dim = 1
u0_aug = Float32[u0;zeros(hidden_dim)]

NN = FastChain((x, p) -> x,
                  FastDense(input_dim+hidden_dim, 32, swish),
                  FastDense(32, 32, swish),
                  FastDense(32, input_dim+hidden_dim))

prob_neuralode = NeuralODE(NN, tspan, Tsit5(), saveat = t)

function predict_neuralode(p)
  Array(prob_neuralode(u0_aug, p))
end
```

We only want the loss function to depend on the input dimension hence:

```{julia, eval=false}
function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, sol_lotka .- pred[1:input_dim,:])
    return loss, pred
end
```

We then train again and modify the callback function to include the added
extra dimension:


```{julia, eval=false}
iter = 0
loss_vec = Array{Float32}(undef,max_iter+1)
anim_lotka_aug = Animation()
callback = function (p, l, pred)
  global list_plots, iter, anim_lotka

  # response in training progression
  if iter%50 == 0
    display("Loss of iteration $iter: $l")
  end

  # save loss (+1 as julia is 1-indexed)
  loss_vec[iter+1] = l

  # plot current prediction against data
  plt = plot(t, sol_lotka', label = ["x" "y"],seriestype = :scatter,
                legend=:bottomright,color=[:blue :red],xlabel="t",ylims=(-4,5.5))
  plot!(plt, t, pred', label = ["x NODE" "y NODE" "Aug dim"],color=[:blue :red :green])
  frame(anim_lotka_aug)

  iter += 1
  return false
end
result_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                          ADAM(0.05), cb = callback,
                                          maxiters = max_iter);
```


We now plot the loss function

```{julia,eval=false}
plt_loss = plot(loss_vec,label="log loss", title="Log loss function of 1-dim augmented",
                xlabel="Iteration",yaxis=:log,ylabel="log loss")
png(plt_loss,raw"ANODE\Figures\loss_lotka-volterra_aug_1dim.png")
```
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/loss_lotka-volterra_aug_1dim.png?raw=true)

we don't have the same huge spikes in the loss function, however,
we still have long periods where we get caught. We now display how the trajectories change
for each training epoch:

```{julia, eval=false}
gif(anim_lotka_aug,raw"ANODE\Figures\lotka-volterra_aug_1dim.gif",fps=15)
```
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/lotka-volterra_aug_1dim.gif?raw=true)

Again the system finds itself caught in the average of the oscillations and sporadically
finds new minima.

###### Lotka-Volterra with ANODE and $p=3$

We now add $3$ dimensions to see how it changes our training pattern

```{julia, eval=false}
input_dim = size(u0)[1]
hidden_dim = 3
u0_aug = Float32[u0;zeros(hidden_dim)]

NN = FastChain((x, p) -> x,
                  FastDense(input_dim+hidden_dim, 32, swish),
                  FastDense(32, 32, swish),
                  FastDense(32, input_dim+hidden_dim))

prob_neuralode = NeuralODE(NN, tspan, Tsit5(), saveat = t)

function predict_neuralode(p)
  Array(prob_neuralode(u0_aug, p))
end

function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, sol_lotka .- pred[1:input_dim,:])
    return loss, pred
end

iter = 0
loss_vec = Array{Float32}(undef,max_iter+1)
anim_lotka_aug_3 = Animation()
callback = function (p, l, pred)
  global list_plots, iter, anim_lotka

  # response in training progression
  if iter%50 == 0
    display("Loss of iteration $iter: $l")
  end

  # save loss (+1 as julia is 1-indexed)
  loss_vec[iter+1] = l

  # plot current prediction against data
  plt = plot(t, sol_lotka', label = ["x" "y"],seriestype = :scatter,
                legend=:bottomright,color=[:blue :red],xlabel="t",ylims=(-4,5.5))
  plot!(plt, t, pred', label = ["x NODE" "y NODE" "Aug dim 1" "Aug dim 2" "Aug dim 3"]
        ,color=[:blue :red :green :skyblue1 :darkorchid1])
  frame(anim_lotka_aug_3)

  iter += 1
  return false
end
result_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,
                                          ADAM(0.05), cb = callback,
                                          maxiters = max_iter);
```

```{julia,eval=false}
plt_loss = plot(loss_vec,label="log loss", title="Log loss function of 3-dim augmented",
                xlabel="Iteration",yaxis=:log,ylabel="log loss")
png(plt_loss,raw"ANODE\Figures\loss_lotka-volterra_aug_3dim.png")
```
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/loss_lotka-volterra_aug_3dim.png?raw=true)

loss function decreases more steadily without the same huge spikes. The set
of possible parameters for the network have increased which we could reason
means that the `ADAM` optimizer does not need to radically try new combinations.

**The animation** shows:

```{julia, eval=false}
gif(anim_lotka_aug_3,raw"ANODE\Figures\lotka-volterra_aug_3dim.gif",fps=15)
```
![](https://github.com/NicolajHMNielsen/SciML_DTU/blob/main/ANODE/Figures/lotka-volterra_aug_3dim.gif?raw=true)

The dynamics of the training look much different and look more stable than
the training of the NODE and the ANODE with 1 added dimension when we focus only
on the trajectories `x NODE` and `y NODE`. Somehow it seems that the optimizer can more
easily continuously deform the trajectories.

**In this second part** we have seen how the training pattern changes when we use
ANODE and what the change of dynamics looks like when we train. It seems that
the extra added dimensions really create more flexibility for the optimizer
which is very important to find the best fit.

##### Question
What is going on in the extra dimension and do they resemble the dynamics of a
hidden variable?

In [notebook](https://nicolajhmnielsen.github.io/SciML_DTU/ANODE/HTML/Hidden_bears)
we investigate this and add another variable to the Lotka Volterra system.
Then we will hide the data of the last variable and train an ANODE with one added dimension on the
first two. We then address if the ANODE can encapsulate the dynamics of the system and
if the dynamics in the added dimension resemble the dynamics of the missing variable.

#### References
[^Perko]: Perko, Lawrence, *Differential equations and dynamical systems*, Vol 7 Springer Science & Business Media, 2013
[^Dupont]: Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye, *Augmented Neural ODEs*, arXiv preprint arXiv:1904.01681, 2019
[^Strauss]: Strauss, Robert *Augmenting Neural Differential Equations to Model Unknown Dynamical Systems with Incomplete State Information*, arXiv:2008.08226, 2020
